ceph:
  version: "reef"
  image_version: v18 # used by iscsi gateway
  
  mon:
    count: 3 # deploy count daemon across any host
    # TODO: currently we require explicitely define hosts,
    #       as used by deploy_cluster.yml to check state
    hosts:
      - ceph01
      - ceph02
      - ceph03

  osd:
    storage_types: # this defines crush maps `replication_<type>` which maps to specific disk tiers
      - type: ssd
      # - type: nvme
  
  rbd:
    pools: # RBD pools
      # Compute (oVirt) pools used by iSCSI gateway:
      - name: ovirt-ssd
        type: ssd
        used_by: compute
      # Storage v2 with mulitple pools:
      - name: iscsi-ssd
        type: ssd
        used_by: storage
        ceph_user: storage-iscsi-ssd
      # Pools for OKD clusters:
      - name: openshift-sys1
        type: ssd
        used_by: openshift
        ceph_user: openshift-admin
      - name: openshift-orc1
        type: ssd
        used_by: openshift
        ceph_user: openshift-admin

  mds: # aka cephfs
    volumes:
    - name: openshift
      type: ssd
    - name: nextcloud
      type: ssd

  iscsi:
    configs:
    - id: ssd # id of service
      pool: ovirt-ssd # where config is stored
      hosts:
        - ceph01
        - ceph02
      targets:
        # It's recommended to have separate target for hosted-engine datastore
        # AKZ: consumes hosted-engine datastore from PureStorage FlashArray
        # - name: he
        #   host_group: ovih_san
        #   disks:
        #     - name: he
        #       pool: ovirt-ssd
        #       size: 150G
        - name: ovirt-ssd
          host_group: ovih_san
          disks:
            - name: data-ssd-1
              pool: ovirt-ssd
              size: 20T
            - name: data-ssd-2
              pool: ovirt-ssd
              size: 20T

  s3:
    count: 3 # deploy count daemon on any hosts
    #hosts:
    #  - ceph01
    #  - ceph02
    #  - ceph03
    # port: 81
    pools: # there is rgw pools described, each rgw pool consist of 3 rados pools
      - type: ssd
        name: buckets     # it's default placement used for internal usage
      # Other pools used by Storage service
      - type: ssd
        # name: ssd      # by default inherited from .type
  # Ingress service is supposed to be used primarly by S3 RGW and prometheus
  ingress:
    # vip address configures in inventory file
    count: 3 # deploy count daemon on any hosts
    #hosts:
    #  - ceph01
    #  - ceph02
    #  - ceph03
    # port: 80
    # mon_port: 1967
    # dashboard_port: 4443
    # mgr_prometheus_port: 9285
    # restful_port: 9286

# Was used only for Storage V1:
#storage_service:
#  prometheus_ha_port: 9285
#  ceph_user: storage
#  bucket: storage-db
