## Hardware access
#[seed_machine]
#seed-vm ansible_host=
#[seed_machine:vars]
#nfs_server=10.10.65.92
#nfs_subnet=10.11.70.0/24
#ansible_user=root
#ansible_ssh_common_args='-o Port=8149'
#ansible_python_interpreter=/usr/bin/python3

[oc_controller]
localhost ansible_connection=local

[all:vars]
ansible_ssh_common_args='-o ProxyJump=root@10.253.17.1 -o StrictHostKeyChecking=no'
ansible_user=root

## This location does not have automation for dataswitches
[data_switches]
# sensitive to order
#datasw-1 ansible_host=10.25.248.11
#datasw-2 ansible_host=10.25.248.12
[data_switches:vars]
#ansible_port=22
#ansible_user=dsatsura
#ansible_net_username=dsatsura
#ansible_connection=ansible.netcommon.network_cli
#ansible_network_os=cisco.nxos.nxos
#ansible_ssh_common_args='-o ProxyCommand="ssh -W %h:%p -q sshproxy@172.20.12.104"'
#ansible_host_key_checking=false
#ansible_python_interpreter=/usr/bin/python

## Out-of-Band Management (OOBM)
## aka Lights-Out Management (LOM)
## vendor implementations: iDRAC, iLO, IMM
[ovih_oobm]
ovih01-oobm ansible_host=10.98.1.15
ovih02-oobm ansible_host=10.98.1.5
ovih03-oobm ansible_host=10.98.1.11
ovih04-oobm ansible_host=10.98.1.13
ovih05-oobm ansible_host=10.98.1.12
[ovih_oobm:vars]
#user_oobm=root

[ceph_oobm]
# This location have Ceph host on VMs, so do not have oobm interfaces
#ceph01-oobm ansible_host=10.16.52.0

## Operating System Management
## used to configure OS with ansible

[ovih_osm]
ovih01-osm ansible_host=10.99.1.20
ovih02-osm ansible_host=10.99.1.2
ovih03-osm ansible_host=10.99.1.18
#ovih04-osm ansible_host=10.99.1.3
#ovih05-osm ansible_host=10.99.1.12
[ovih_osm:vars]
h_e_domain_type=iscsi
h_e_iscsi_target=iqn.2010-06.com.purestorage:flasharray.1feaafaf753bbe90
h_e_storage_domain_addr=10.254.6.2
h_e_mem_size_MB=16384
h_e_vcpus=12
# he_lun_id=3624a937092639b507bf44270000113ea # auto-detect
## This location do not have OOB module automation for installing OS
#os_disk=sda
#interface=eno1
#netmask=255.255.255.0
gateway=10.99.1.1
ansible_ssh_common_args='-o ProxyJump=seed.akz -o StrictHostKeyChecking=no'
ansible_port=22
ansible_user=root

[ceph_vm_osm]
ceph01-osm ansible_host=10.254.20.6
ceph02-osm ansible_host=10.254.20.7
ceph03-osm ansible_host=10.254.20.8
[ceph_vm_osm:vars]
[ceph_fast_osm]
[ceph_fast_osm:vars]
#use_lldp=no
#[ceph_slow_osm]
#[ceph_slow_osm:vars]
#use_lldp=no
[ceph_osm:children]
ceph_vm_osm # We use ceph on Virtual Machines, so we do not have separate OSM interface
#ceph_fast_osm
#ceph_slow_osm
[ceph_osm:vars]
#os_disk=sda
#interface=eno1
#netmask=255.255.255.0
#gateway=
#ansible_ssh_common_args='-o ProxyJump='
ansible_user=root

## oVirt interfaces
[ovirt_engine]
engine-ovirt ansible_host=10.254.1.20
[ovirt_engine:vars]
ansible_ssh_common_args='-o ProxyJump=seed.akz -o StrictHostKeyChecking=no'
ansible_user=root
iscsi_target=iqn.2010-06.com.purestorage:flasharray.1feaafaf753bbe90
iscsi_address=10.254.6.2
# here we should define full hostname
[ovirt_engine_osm]
engine-ovirt.akz.icdc.io ansible_ssh_common_args='-o ProxyJump=seed.akz'

[ovih_virtgw]
ovih01-virtgw ansible_host=10.254.0.21
ovih02-virtgw ansible_host=10.254.0.22
ovih03-virtgw ansible_host=10.254.0.23
ovih04-virtgw ansible_host=10.254.0.24
ovih05-virtgw ansible_host=10.254.0.25

[ovih_he_mgmt]
ovih01-mgmt ansible_host=10.254.1.21
ovih02-mgmt ansible_host=10.254.1.22
ovih03-mgmt ansible_host=10.254.1.23
[ovih_he_mgmt:vars]
ansible_ssh_common_args='-o ProxyJump=seed.akz -o StrictHostKeyChecking=no'
ansible_user=root

[ovih_mgmt:children]
ovih_he_mgmt
[ovih_mgmt]
#ovih04-mgmt ansible_host=10.254.1.24
#ovih05-mgmt ansible_host=10.254.1.25
[ovih_mgmt:vars]
ansible_ssh_common_args='-o ProxyJump=seed.akz'
ansible_user=root

[ovih_he_san]
ovih01-san ansible_host=10.254.2.21
ovih02-san ansible_host=10.254.2.22
ovih03-san ansible_host=10.254.2.23
[ovih_he_san:vars]
ansible_ssh_common_args='-o ProxyJump=seed.akz'
ansible_user=root

[ovih_san:children]
ovih_he_san
[ovih_san]
ovih04-san ansible_host=10.254.2.24
ovih05-san ansible_host=10.254.2.25
[ovih_san:vars]
ansible_ssh_common_args='-o ProxyJump=seed.akz'
ansible_user=root

[ovih_display]
ovih01-display ansible_host=10.254.4.21
ovih02-display ansible_host=10.254.4.22
ovih03-display ansible_host=10.254.4.23
#ovih04-display ansible_host=10.254.4.24
#ovih05-display ansible_host=10.254.4.25

[ovih_migration]
ovih01-migration ansible_host=10.254.5.21
ovih02-migration ansible_host=10.254.5.22
ovih03-migration ansible_host=10.254.5.23
ovih04-migration ansible_host=10.254.5.24
ovih05-migration ansible_host=10.254.5.25

# Optional groups
# External storage currenntly support by 2 path: iscsi01, iscsi02
[ovih_iscsi01]
ovih01-iscsi01 ansible_host=10.254.6.21
ovih02-iscsi01 ansible_host=10.254.6.22
ovih03-iscsi01 ansible_host=10.254.6.23
ovih04-iscsi01 ansible_host=10.254.6.24
ovih05-iscsi01 ansible_host=10.254.6.25
[ovih_iscsi02]
ovih01-iscsi02 ansible_host=10.254.7.21
ovih02-iscsi02 ansible_host=10.254.7.22
ovih03-iscsi02 ansible_host=10.254.7.23
ovih04-iscsi02 ansible_host=10.254.7.24
ovih05-iscsi02 ansible_host=10.254.7.25
[ovih_vgpu]
[ovih_nestedvt]
ovih04-nestedvt
ovih05-nestedvt

[display_proxy]
display-1 ansible_host=10.254.20.5
[display_proxy_external]
display-1-external ansible_host=194.164.115.4

## Ceph interfaces

[ceph_mgmt]
# We have Ceph on Virtual Machines, so we place them in base network (not mgmt)
# mgmt vlan is used by he-bridge network and is dedicated exclusively for hosted-engine VM.
ceph01-mgmt ansible_host=10.254.20.6
ceph02-mgmt ansible_host=10.254.20.7
ceph03-mgmt ansible_host=10.254.20.8
#ceph04-mgmt ansible_host=10.254.1.14
#ceph05-mgmt ansible_host=10.254.1.15
[ceph_mgmt:vars]
# need for iscsigw CloudGateways
ansible_ssh_common_args='-o ProxyJump=seed.akz -o StrictHostKeyChecking=no'
ansible_port=22
ansible_user=root
disk_type=ssd
disk_count=23

[ceph_san]
ceph01-san ansible_host=10.254.2.11
ceph02-san ansible_host=10.254.2.12
ceph03-san ansible_host=10.254.2.13
#ceph04-san ansible_host=10.254.2.14
#ceph05-san ansible_host=10.254.2.15

[ceph_vip]
ceph-ingress ansible_host=10.254.2.10

[ceph_replica]
ceph01-replica ansible_host=10.254.3.11
ceph02-replica ansible_host=10.254.3.12
ceph03-replica ansible_host=10.254.3.13
#ceph04-replica ansible_host=10.254.3.14
#ceph05-replica ansible_host=10.254.3.15

############################################
# Infrastructure Cluster
# Runs CoreDNS
# Configure IP addresses here, usually first 3 ip addresses from sys_public
[infra]
infra-1 ansible_host=10.254.20.2
infra-2 ansible_host=10.254.20.3 caching_proxy=true
infra-3 ansible_host=10.254.20.4
[infra:vars]
# go through SSH via public IP
ansible_ssh_common_args='-o ProxyJump=root@194.164.115.2:2201 -o StrictHostKeyChecking=no'
ansible_port=22
ansible_user=root

# You can override here DNS public address
# [dns]
# dns-external ansible_host=x.x.x.x

#############################################
# Openshift IPs
[openshift]
ocp-01 ansible_host=10.254.29.20
ocp-02 ansible_host=10.254.29.21
ocp-03 ansible_host=10.254.29.22
ocp-04 ansible_host=10.254.29.23
ocp-05 ansible_host=10.254.29.24
ocp-06 ansible_host=10.254.29.25
ocp-07 ansible_host=10.254.29.26
ocp-08 ansible_host=10.254.29.27
ocp-09 ansible_host=10.254.29.28
ocp-10 ansible_host=10.254.29.29
ocp-11 ansible_host=10.254.29.30
ocp-12 ansible_host=10.254.29.31
ocp-13 ansible_host=10.254.29.32
ocp-14 ansible_host=10.254.29.33
ocp-15 ansible_host=10.254.29.34

#############################################
# Cloud Gateway services
#
# SYS account cloudgw has special settings
[sys_cloudgw_account]
cloudgw-84c88afb ansible_host=198.18.240.5 cloudgw_id=84c88afb-4f5f-5f0b-994a-f3ed8e93fdaa router_name=sys_default
# New Cloud Gateway will be automatically added here
[new_cloudgw_account]
[new_cloudgw_account:children]
# cloudgw_account
# sys_cloudgw_account
[new_cloudgw_account:vars]
# before first VPN deployment go through first infra node (port :2201)
ansible_ssh_common_args='-o ProxyJump=root@sys.vpn.akz.icdc.io:2201 -o StrictHostKeyChecking=no'
ansible_port=22
ansible_user=root
pool_name=iscsi-ssd
# Move finalized cloud gateways here
[cloudgw_account]
[cloudgw_account:children]
sys_cloudgw_account
[cloudgw_account:vars]
# before first VPN deployment go through first infra node (port :2201)
ansible_ssh_common_args='-o ProxyJump=root@sys.vpn.akz.icdc.io:2201 -o StrictHostKeyChecking=no'
ansible_port=22
ansible_user=root

# Use for publishing backend in Traefik
[s3]
ceph01-s3 ansible_host=10.254.2.11
ceph02-s3 ansible_host=10.254.2.12
ceph03-s3 ansible_host=10.254.2.13
#ceph04-s3 ansible_host=10.254.2.14
#ceph05-s3 ansible_host=10.254.2.15
#[s3:children]
# ceph_san
[s3_local:children]
# Use for publishing backend in Local Account Traefik accessing through SAN network
ceph_san

[disk:children]
nc
[traefik_manager]
traefik-manager ansible_host=traefik.akz.icdc.io # fake record required to deploy traefik config
[wireguard_manager]
wireguard-manager ansible_host=wireguard.akz.icdc.io # fake record required to deploy traefik config
[etcd_dns]
etcd-dns
# End of fake traefic records

[zabbix]
zabbix-base ansible_host=10.254.20.11
[zabbix_cloudgw]
zabbix-account ansible_host=198.18.240.6
[zabbix:vars]
data_disk=sda

[elk]
elk01-base ansible_host=10.254.20.12
elk02-base ansible_host=10.254.20.14
elk03-base ansible_host=10.254.20.15

[elk_es]
elk01-es
elk02-es
elk03-es

[elk_kibana]
elk01-kibana

[elk_logstash]
elk02-logstash
elk03-logstash

[nc]
nc-base ansible_host=10.254.20.10

[code]
code-base ansible_host=10.254.20.13

[export]
export-base ansible_host=10.254.20.6
[image_catalog]
image-catalog-local ansible_host=10.254.2.10 expose= # currently not exposed to other locations
image-catalog-zeu ansible_host=s3.zeu.icdc.io
